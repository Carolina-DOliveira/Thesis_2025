{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ba9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c6c4c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(895, 40)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_excel(\"895_records_with_descriptors_with_names.xlsx\")\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f658e",
   "metadata": {},
   "source": [
    "##### **Variables Enconding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e3ab22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35507/3772536482.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df: DataFrame = dataset.replace(encoding, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "dissolve_encoding: dict[str, int] = {\"NO\": 0,\"YES\": 1}\n",
    "sample_type_enconding: dict[str, int] = {\"pellet\": 0, \"waste\": 1, \"fiber\": 2, \"film\": 3, \"powder\": 4}\n",
    "\n",
    "encoding: dict[str, dict[str, int]] = {\n",
    "    \"Dissolution\": dissolve_encoding,\n",
    "    \"Sample type\": sample_type_enconding\n",
    "}\n",
    "df: DataFrame = dataset.replace(encoding, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fc9bd",
   "metadata": {},
   "source": [
    "##### **Variables Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f2493f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Polymer and Solvent Identifier Columns\n",
    "df = df.drop(columns=[\"Polymer_ID\", \"Solvent_ID\", \"Polymer\", \"Solvent\"])\n",
    "\n",
    "# Separate features (X) from target \"Dissolution\" (y)\n",
    "X = df.drop(columns=[\"Dissolution\"])\n",
    "y = df[\"Dissolution\"]\n",
    "\n",
    "# Apply log(1+x) Transformation\n",
    "X_log = np.log1p(X)\n",
    "\n",
    "# Scale to [0,1] with MinMax Normalization\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "X_scaled = min_max_scaler.fit_transform(X_log)\n",
    "\n",
    "# Rebuild Dataset merging normalized features with target\n",
    "df_log_minmax = DataFrame(X_scaled, columns = X.columns, index= X.index)\n",
    "df_log_minmax[\"Dissolution\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5802cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) from target \"Dissolve\" (y)\n",
    "X = df_log_minmax.drop(columns=[\"Dissolution\"])\n",
    "y = df_log_minmax[\"Dissolution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d749b",
   "metadata": {},
   "source": [
    "##### **Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "param_grid = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 12,\n",
    "    'max_features': 0.5,\n",
    "    'max_leaf_nodes': 123,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'splitter': 'best'\n",
    "}\n",
    "\n",
    "# Save Results\n",
    "acc_val_list = []\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "recall_test_list = []\n",
    "best_params_list = []\n",
    "\n",
    "# Loop of Iterations with random_state varying the data split \n",
    "for i in range(5): \n",
    "    # Split data into Train (70%)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=i, stratify=y)\n",
    "    # Split the rest of the data into Validation (15%) and Test (15%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=i, stratify=y_temp)\n",
    "\n",
    "    print(f\"\\n{'='*30}\\nStarting Iteration {i+1}\\n{'='*30}\")\n",
    "\n",
    "    # Train model with hyperparameters\n",
    "    model = DecisionTreeClassifier(**param_grid, random_state=i)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute Metrics\n",
    "    acc_val = accuracy_score(y_val, y_val_pred)\n",
    "    acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Store metrics\n",
    "    acc_val_list.append(acc_val)\n",
    "    acc_train_list.append(acc_train)\n",
    "    acc_test_list.append(acc_test)\n",
    "\n",
    "    # Show iteration results\n",
    "    print(f\"Running parameters:  {param_grid}\")\n",
    "    print(f\"Accuracy - Validation:  {acc_val:.4f}\")\n",
    "    print(f\"Accuracy - Train:     {acc_train:.4f}\")\n",
    "    print(f\"Accuracy - Test:      {acc_test:.4f}\")\n",
    "\n",
    "\n",
    "# Compute mean and standard deviation after all iterations\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"Accuracy - Validation: Mean = {np.mean(acc_val_list):.4f}, Standard Deviation = {np.std(acc_val_list):.4f}\")\n",
    "print(f\"Accuracy - Train:     Mean = {np.mean(acc_train_list):.4f}, Standard Deviation = {np.std(acc_train_list):.4f}\")\n",
    "print(f\"Accuracy - Test:      Mean = {np.mean(acc_test_list):.4f}, Standard Deviation = {np.std(acc_test_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7310a40",
   "metadata": {},
   "source": [
    "##### **Gradient Boosting Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to test\n",
    "param_grid = {\n",
    "    'learning_rate': 0.1,\n",
    "    'loss': 'exponential',\n",
    "    'max_depth': 10,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 10,\n",
    "    'n_estimators': 250,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "# Save Results\n",
    "acc_val_list = []\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "recall_test_list = []\n",
    "best_params_list = []\n",
    "\n",
    "# Loop of Iterations with random_state varying the data split \n",
    "for i in range(5): \n",
    "    # Split data into Train (70%)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=i, stratify=y)\n",
    "    # Split the rest of the data into Validation (15%) and Test (15%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=i, stratify=y_temp)\n",
    "\n",
    "    print(f\"\\n{'='*30}\\nStarting Iteration {i+1}\\n{'='*30}\")\n",
    "\n",
    "    # Train model with hyperparameters\n",
    "    model = GradientBoostingClassifier(**param_grid, random_state=i)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute Metrics\n",
    "    acc_val = accuracy_score(y_val, y_val_pred)\n",
    "    acc_train = accuracy_score(y_train, y_train_pred)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Store metrics\n",
    "    acc_val_list.append(acc_val)\n",
    "    acc_train_list.append(acc_train)\n",
    "    acc_test_list.append(acc_test)\n",
    "\n",
    "    # Show iteration results\n",
    "    print(f\"Running parameters:  {param_grid}\")\n",
    "    print(f\"Accuracy - Validation:  {acc_val:.4f}\")\n",
    "    print(f\"Accuracy - Train:     {acc_train:.4f}\")\n",
    "    print(f\"Accuracy - Test:      {acc_test:.4f}\")\n",
    "\n",
    "\n",
    "# Compute mean and standard deviation after all iterations\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"Accuracy - Validation: Mean = {np.mean(acc_val_list):.4f}, Standard Deviation = {np.std(acc_val_list):.4f}\")\n",
    "print(f\"Accuracy - Train:     Mean = {np.mean(acc_train_list):.4f}, Standard Deviation = {np.std(acc_train_list):.4f}\")\n",
    "print(f\"Accuracy - Test:      Mean = {np.mean(acc_test_list):.4f}, Standard Deviation = {np.std(acc_test_list):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
